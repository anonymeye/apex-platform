# Conduit Python Port - Agent Implementation Plan

> **Target Audience**: AI Coding Agents
> 
> **Purpose**: Step-by-step implementation guide with clear tasks, acceptance criteria, and validation steps
>
> **Status**: Ready for Execution

---

## üìã Table of Contents

1. [Overview](#overview)
2. [Prerequisites](#prerequisites)
3. [Phase-by-Phase Implementation](#phase-by-phase-implementation)
4. [Task Execution Guidelines](#task-execution-guidelines)
5. [Validation & Testing](#validation--testing)
6. [Common Issues & Solutions](#common-issues--solutions)

---

## Overview

### Project Goal

Port Conduit from Clojure to Python, creating a simple, type-safe, functional LLM orchestration library.

### Success Criteria

- ‚úÖ 100% type coverage (mypy strict mode)
- ‚úÖ >85% test coverage (>90% for core modules)
- ‚úÖ All providers working (OpenAI, Anthropic, Groq minimum)
- ‚úÖ Complete RAG pipeline functional
- ‚úÖ Agent tool loops operational
- ‚úÖ Comprehensive documentation
- ‚úÖ Published to PyPI

### Timeline Estimate

- **Aggressive**: 12-16 weeks (full-time)
- **Conservative**: 6-9 months (part-time)

---

## Prerequisites

### Required Knowledge

1. Python 3.11+ features (type hints, async/await)
2. Pydantic v2 (models, validation, JSON schema)
3. httpx (async HTTP client)
4. pytest (async testing)
5. Understanding of LLM APIs (OpenAI, Anthropic)

### Required Tools

```bash
# Python 3.11 or higher
python --version  # Should be 3.11+

# Poetry for dependency management
pip install poetry

# Development tools
pip install mypy ruff black pytest pytest-asyncio
```

### Reference Documents

Read these in order:
1. `PYTHON_PORT_SUMMARY.md` - Overview and feasibility
2. `PYTHON_PORT_GUIDE.md` - Complete implementation guide
3. `PYTHON_PORT_QUICK_REFERENCE.md` - Quick lookup
4. `PYTHON_PORT_ROADMAP.md` - Project phases

---

## Phase-by-Phase Implementation

## Phase 0: Project Setup

**Duration**: 1-2 days

### Task 0.1: Initialize Repository

**Objective**: Set up GitHub repository and basic project structure

**Steps**:
1. Create GitHub repository: `conduit-py`
2. Initialize with README, LICENSE (MIT), .gitignore
3. Create branch structure: `main`, `develop`
4. Set up GitHub Actions for CI/CD

**Deliverables**:
- GitHub repository URL
- Initial README with project description
- MIT License file
- Python .gitignore

**Validation**:
```bash
git clone <repo-url>
cd conduit-py
ls -la  # Should see README.md, LICENSE, .gitignore
```

---

### Task 0.2: Create Project Structure

**Objective**: Set up directory structure and Poetry configuration

**Steps**:
1. Create directory structure:
```bash
mkdir -p src/conduit/{core,schema,providers,interceptors,tools,agent,rag,memory,flow,streaming,structured,errors,utils}
mkdir -p tests/{unit,integration,e2e}
mkdir -p examples
mkdir -p docs
```

2. Create `pyproject.toml`:
```toml
[tool.poetry]
name = "conduit-py"
version = "0.1.0"
description = "Simple, functional LLM orchestration for Python"
authors = ["Your Name <you@example.com>"]
license = "MIT"
readme = "README.md"
packages = [{include = "conduit", from = "src"}]

[tool.poetry.dependencies]
python = "^3.11"
pydantic = "^2.0"
httpx = "^0.27"
numpy = "^1.26"
structlog = "^24.0"

[tool.poetry.group.dev.dependencies]
pytest = "^8.0"
pytest-asyncio = "^0.23"
pytest-cov = "^4.1"
pytest-httpx = "^0.30"
mypy = "^1.8"
ruff = "^0.1"
black = "^24.0"

[tool.black]
line-length = 100
target-version = ["py311"]

[tool.ruff]
line-length = 100
target-version = "py311"
select = ["E", "F", "I", "B", "UP", "ANN"]

[tool.mypy]
python_version = "3.11"
strict = true
warn_return_any = true
warn_unused_configs = true

[tool.pytest.ini_options]
asyncio_mode = "auto"
testpaths = ["tests"]

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"
```

3. Create `src/conduit/__init__.py` with version
4. Create `src/conduit/py.typed` (empty file for PEP 561)
5. Create `tests/conftest.py` with common fixtures

**Deliverables**:
- Complete directory structure
- `pyproject.toml` configured
- `py.typed` marker file
- Initial `__init__.py` files

**Validation**:
```bash
poetry install
poetry run pytest --version
poetry run mypy --version
poetry run black --version
```

---

### Task 0.3: Configure CI/CD

**Objective**: Set up automated testing and quality checks

**Steps**:
1. Create `.github/workflows/test.yml`:
```yaml
name: Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.11", "3.12"]
    
    steps:
    - uses: actions/checkout@v3
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    - name: Install dependencies
      run: |
        pip install poetry
        poetry install
    - name: Run tests
      run: poetry run pytest --cov=conduit --cov-report=xml
    - name: Type check
      run: poetry run mypy src/conduit
    - name: Lint
      run: poetry run ruff check src/conduit
```

**Deliverables**:
- GitHub Actions workflow for tests
- GitHub Actions workflow for linting
- GitHub Actions workflow for type checking

**Validation**:
- Push to GitHub and verify Actions run
- Check that all checks pass (even with empty code)

---

## Phase 1: Core Foundation

**Duration**: 1-2 weeks

### Task 1.1: Implement Error Classes

**Objective**: Define all exception types

**File**: `src/conduit/errors/base.py`

**Implementation**:
```python
"""Core exception classes for Conduit."""


class ConduitError(Exception):
    """Base exception for all Conduit errors."""
    pass


class ProviderError(ConduitError):
    """Error from LLM provider API."""
    
    def __init__(self, message: str, provider: str | None = None, status_code: int | None = None):
        super().__init__(message)
        self.provider = provider
        self.status_code = status_code


class AuthenticationError(ProviderError):
    """Authentication failed with provider."""
    pass


class RateLimitError(ProviderError):
    """Rate limit exceeded."""
    
    def __init__(self, message: str, retry_after: float | None = None):
        super().__init__(message)
        self.retry_after = retry_after


class ValidationError(ConduitError):
    """Data validation error."""
    pass


class TimeoutError(ConduitError):
    """Request timeout."""
    pass


class MaxIterationsError(ConduitError):
    """Agent exceeded maximum iterations."""
    pass


class ToolExecutionError(ConduitError):
    """Error executing tool."""
    
    def __init__(self, message: str, tool_name: str):
        super().__init__(message)
        self.tool_name = tool_name
```

**Tests**: `tests/unit/test_errors.py`
```python
"""Tests for error classes."""

import pytest
from conduit.errors import (
    ConduitError,
    ProviderError,
    AuthenticationError,
    RateLimitError,
)


def test_conduit_error():
    """Test base error."""
    error = ConduitError("test error")
    assert str(error) == "test error"
    assert isinstance(error, Exception)


def test_provider_error():
    """Test provider error with metadata."""
    error = ProviderError("API error", provider="openai", status_code=500)
    assert error.provider == "openai"
    assert error.status_code == 500


def test_authentication_error():
    """Test authentication error."""
    error = AuthenticationError("Invalid key")
    assert isinstance(error, ProviderError)


def test_rate_limit_error():
    """Test rate limit error with retry_after."""
    error = RateLimitError("Rate limited", retry_after=60.0)
    assert error.retry_after == 60.0
```

**Acceptance Criteria**:
- ‚úÖ All error classes defined
- ‚úÖ Proper inheritance hierarchy
- ‚úÖ Metadata fields included
- ‚úÖ 100% test coverage
- ‚úÖ Type hints complete

**Validation**:
```bash
poetry run pytest tests/unit/test_errors.py -v
poetry run mypy src/conduit/errors
```

---

### Task 1.2: Implement Schema Models

**Objective**: Define all Pydantic models for messages, responses, and options

**File**: `src/conduit/schema/messages.py`

**Implementation**:
```python
"""Message schemas for Conduit."""

from typing import Literal, Union
from pydantic import BaseModel, Field


class TextBlock(BaseModel):
    """Text content block."""
    type: Literal["text"] = "text"
    text: str = Field(..., min_length=1)


class ImageSource(BaseModel):
    """Image source (URL or base64)."""
    type: Literal["url", "base64"]
    url: str | None = None
    data: str | None = None
    media_type: Literal["image/jpeg", "image/png", "image/gif", "image/webp"] | None = None


class ImageBlock(BaseModel):
    """Image content block."""
    type: Literal["image"] = "image"
    source: ImageSource


ContentBlock = Union[str, TextBlock, ImageBlock]


class Message(BaseModel):
    """Chat message.
    
    Examples:
        >>> Message(role="user", content="Hello!")
        >>> Message(role="assistant", content="Hi there!")
    """
    role: Literal["user", "assistant", "system", "tool"]
    content: str | list[ContentBlock]
    name: str | None = None
    tool_call_id: str | None = None
    
    model_config = {
        "json_schema_extra": {
            "examples": [
                {"role": "user", "content": "Hello!"},
                {"role": "assistant", "content": "Hi there!"},
            ]
        }
    }
```

**File**: `src/conduit/schema/responses.py`

**Implementation**:
```python
"""Response schemas for Conduit."""

from typing import Literal
from pydantic import BaseModel, Field
from conduit.schema.messages import ContentBlock, TextBlock


class Usage(BaseModel):
    """Token usage information."""
    input_tokens: int = Field(..., ge=0)
    output_tokens: int = Field(..., ge=0)
    total_tokens: int | None = Field(None, ge=0)
    cache_read_tokens: int | None = Field(None, ge=0)
    cache_creation_tokens: int | None = Field(None, ge=0)


class FunctionCall(BaseModel):
    """Function call details."""
    name: str
    arguments: dict


class ToolCall(BaseModel):
    """Tool invocation requested by the model."""
    id: str
    type: Literal["function"] = "function"
    function: FunctionCall


class Response(BaseModel):
    """Unified response format from all providers."""
    id: str | None = None
    role: Literal["assistant"] = "assistant"
    content: str | list[ContentBlock]
    model: str | None = None
    stop_reason: Literal["end_turn", "tool_use", "max_tokens", "stop_sequence", "content_filter"] | None = None
    usage: Usage
    tool_calls: list[ToolCall] | None = None
    
    def extract_content(self) -> str:
        """Extract text content from response."""
        if isinstance(self.content, str):
            return self.content
        
        texts = []
        for block in self.content:
            if isinstance(block, str):
                texts.append(block)
            elif isinstance(block, TextBlock):
                texts.append(block.text)
        return "".join(texts)
```

**File**: `src/conduit/schema/options.py`

**Implementation**:
```python
"""Chat options schema."""

from pydantic import BaseModel, Field


class ChatOptions(BaseModel):
    """Options for chat requests."""
    temperature: float | None = Field(None, ge=0.0, le=2.0)
    max_tokens: int | None = Field(None, ge=1)
    top_p: float | None = Field(None, ge=0.0, le=1.0)
    stop: list[str] | None = None
    tools: list | None = None
    tool_choice: str | dict | None = None
    response_format: dict | None = None
```

**Tests**: `tests/unit/test_schema.py`

**Acceptance Criteria**:
- ‚úÖ All schema models defined
- ‚úÖ Validation rules implemented
- ‚úÖ Helper methods included
- ‚úÖ Examples in docstrings
- ‚úÖ 100% test coverage
- ‚úÖ Pydantic validation working

**Validation**:
```bash
poetry run pytest tests/unit/test_schema.py -v
poetry run mypy src/conduit/schema
```

---

### Task 1.3: Implement Core Protocols

**Objective**: Define abstract base classes for providers

**File**: `src/conduit/core/protocols.py`

**Implementation**:
```python
"""Core protocols for Conduit."""

from abc import ABC, abstractmethod
from typing import AsyncIterator
from conduit.schema.messages import Message
from conduit.schema.responses import Response
from conduit.schema.options import ChatOptions


class ModelInfo(BaseModel):
    """Model information."""
    provider: str
    model: str
    capabilities: "Capabilities"


class Capabilities(BaseModel):
    """Model capabilities."""
    streaming: bool = True
    tool_calling: bool = False
    vision: bool = False
    json_mode: bool = False
    max_context: int = 4096


class ChatModel(ABC):
    """Abstract base class for chat models."""
    
    @abstractmethod
    async def chat(
        self,
        messages: list[Message],
        options: ChatOptions | None = None
    ) -> Response:
        """Send messages to the model and receive a response."""
        ...
    
    @abstractmethod
    async def stream(
        self,
        messages: list[Message],
        options: ChatOptions | None = None
    ) -> AsyncIterator[dict]:
        """Stream response from the model."""
        ...
    
    @abstractmethod
    def model_info(self) -> ModelInfo:
        """Get information about this model."""
        ...


class Embeddable(ABC):
    """Abstract base class for embedding models."""
    
    @abstractmethod
    async def embed(
        self,
        texts: str | list[str],
        options: dict | None = None
    ) -> dict:
        """Generate embeddings for text(s)."""
        ...
```

**Tests**: `tests/unit/test_protocols.py`

**Acceptance Criteria**:
- ‚úÖ All protocols defined
- ‚úÖ Complete type hints
- ‚úÖ Comprehensive docstrings
- ‚úÖ Abstract methods marked correctly

**Validation**:
```bash
poetry run mypy src/conduit/core/protocols.py --strict
```

---

## Phase 2: First Provider (OpenAI)

**Duration**: 1 week

### Task 2.1: Implement OpenAI Provider

**Objective**: Create fully functional OpenAI provider

**File**: `src/conduit/providers/openai.py`

**Implementation**: See PYTHON_PORT_GUIDE.md Module 3 for complete template

**Key Requirements**:
1. Implement `ChatModel` protocol
2. Support chat, streaming, tool calling
3. Handle all OpenAI-specific errors
4. Transform between Conduit and OpenAI formats
5. Support async context manager
6. Parse SSE for streaming

**Tests**: `tests/integration/test_openai.py`

**Acceptance Criteria**:
- ‚úÖ All ChatModel methods implemented
- ‚úÖ Streaming works correctly
- ‚úÖ Tool calling supported
- ‚úÖ Error handling complete
- ‚úÖ >85% test coverage
- ‚úÖ Integration tests with mocked API

**Validation**:
```bash
poetry run pytest tests/integration/test_openai.py -v
poetry run mypy src/conduit/providers/openai.py
```

---

### Task 2.2: Implement Mock Provider

**Objective**: Create mock provider for testing

**File**: `src/conduit/providers/mock.py`

**Implementation**:
```python
"""Mock provider for testing."""

from typing import AsyncIterator
from conduit.core.protocols import ChatModel, ModelInfo, Capabilities
from conduit.schema.messages import Message
from conduit.schema.responses import Response, Usage
from conduit.schema.options import ChatOptions


class MockModel(ChatModel):
    """Mock model for testing."""
    
    def __init__(
        self,
        *,
        response_content: str = "Mock response",
        error: Exception | None = None,
        delay: float = 0.0
    ):
        self.response_content = response_content
        self.error = error
        self.delay = delay
        self.call_count = 0
    
    async def chat(
        self,
        messages: list[Message],
        options: ChatOptions | None = None
    ) -> Response:
        """Return mock response."""
        self.call_count += 1
        
        if self.delay > 0:
            await asyncio.sleep(self.delay)
        
        if self.error:
            raise self.error
        
        return Response(
            content=self.response_content,
            usage=Usage(input_tokens=10, output_tokens=5)
        )
    
    async def stream(
        self,
        messages: list[Message],
        options: ChatOptions | None = None
    ) -> AsyncIterator[dict]:
        """Stream mock response."""
        for char in self.response_content:
            yield {"type": "content_delta", "text": char}
    
    def model_info(self) -> ModelInfo:
        """Return mock model info."""
        return ModelInfo(
            provider="mock",
            model="mock-model",
            capabilities=Capabilities()
        )
```

**Acceptance Criteria**:
- ‚úÖ Configurable responses
- ‚úÖ Configurable errors
- ‚úÖ Configurable delays
- ‚úÖ Call counting
- ‚úÖ Useful for testing

---

## Phase 3: Interceptors

**Duration**: 1 week

### Task 3.1: Implement Interceptor Core

**Objective**: Create interceptor execution engine

**Files**:
- `src/conduit/interceptors/base.py`
- `src/conduit/interceptors/context.py`
- `src/conduit/interceptors/execution.py`

**Implementation**: See PYTHON_PORT_GUIDE.md Module 4

**Acceptance Criteria**:
- ‚úÖ Context class complete
- ‚úÖ Interceptor protocol defined
- ‚úÖ Execution chain working
- ‚úÖ Enter/leave/error phases
- ‚úÖ Early termination supported

---

### Task 3.2: Implement Built-in Interceptors

**Objective**: Create commonly used interceptors

**Files**:
- `src/conduit/interceptors/retry.py`
- `src/conduit/interceptors/logging.py`
- `src/conduit/interceptors/cache.py`
- `src/conduit/interceptors/cost_tracking.py`
- `src/conduit/interceptors/rate_limit.py`
- `src/conduit/interceptors/timeout.py`

**Acceptance Criteria**:
- ‚úÖ All 6 interceptors implemented
- ‚úÖ Configurable parameters
- ‚úÖ Proper error handling
- ‚úÖ >90% test coverage

---

## Phase 4: Tools & Agents

**Duration**: 1-2 weeks

### Task 4.1: Implement Tool System

**Objective**: Create tool definition and execution

**Files**:
- `src/conduit/tools/definition.py`
- `src/conduit/tools/execution.py`
- `src/conduit/tools/schema_conversion.py`

**Implementation**: See PYTHON_PORT_GUIDE.md Module 5

**Acceptance Criteria**:
- ‚úÖ Tool class defined
- ‚úÖ Pydantic ‚Üí JSON Schema conversion
- ‚úÖ Tool execution working
- ‚úÖ Error handling complete

---

### Task 4.2: Implement Agent Loop

**Objective**: Create autonomous agent with tool execution

**Files**:
- `src/conduit/agent/loop.py`
- `src/conduit/agent/agent.py`
- `src/conduit/agent/callbacks.py`

**Implementation**: See PYTHON_PORT_GUIDE.md Module 6

**Acceptance Criteria**:
- ‚úÖ Tool loop functional
- ‚úÖ Max iterations enforced
- ‚úÖ Callbacks working
- ‚úÖ Message history maintained
- ‚úÖ Integration tests passing

---

## Phase 5: Additional Providers

**Duration**: 2 weeks

### Task 5.1: Implement Anthropic Provider

**File**: `src/conduit/providers/anthropic.py`

**Acceptance Criteria**:
- ‚úÖ Full ChatModel implementation
- ‚úÖ Claude-specific features
- ‚úÖ Integration tests

---

### Task 5.2: Implement Groq Provider

**File**: `src/conduit/providers/groq.py`

**Acceptance Criteria**:
- ‚úÖ Full ChatModel implementation
- ‚úÖ Fast inference support
- ‚úÖ Integration tests

---

## Phase 6: RAG Pipeline

**Duration**: 2 weeks

### Task 6.1: Implement Text Splitters

**File**: `src/conduit/rag/splitters.py`

**Acceptance Criteria**:
- ‚úÖ Multiple splitter strategies
- ‚úÖ Configurable parameters
- ‚úÖ Unit tests

---

### Task 6.2: Implement Vector Stores

**Files**:
- `src/conduit/rag/stores/base.py`
- `src/conduit/rag/stores/memory.py`

**Acceptance Criteria**:
- ‚úÖ VectorStore protocol
- ‚úÖ In-memory implementation
- ‚úÖ Similarity search working

---

### Task 6.3: Implement RAG Chain

**File**: `src/conduit/rag/chain.py`

**Implementation**: See PYTHON_PORT_GUIDE.md Module 7

**Acceptance Criteria**:
- ‚úÖ End-to-end RAG working
- ‚úÖ Custom templates supported
- ‚úÖ Source tracking
- ‚úÖ Integration tests

---

## Phase 7: Advanced Features

**Duration**: 1-2 weeks

### Task 7.1: Implement Memory Management

**Files**:
- `src/conduit/memory/base.py`
- `src/conduit/memory/conversation.py`
- `src/conduit/memory/windowed.py`

**Acceptance Criteria**:
- ‚úÖ Multiple memory strategies
- ‚úÖ Token-aware windowing
- ‚úÖ Tests passing

---

### Task 7.2: Implement Structured Output

**Files**:
- `src/conduit/structured/output.py`
- `src/conduit/structured/extraction.py`

**Acceptance Criteria**:
- ‚úÖ Schema-based extraction
- ‚úÖ Classification helpers
- ‚úÖ Tests passing

---

## Phase 8: Documentation

**Duration**: 1 week

### Task 8.1: Write Core Documentation

**Files**:
- `README.md`
- `docs/installation.md`
- `docs/quickstart.md`
- `docs/architecture.md`

**Acceptance Criteria**:
- ‚úÖ Clear getting started guide
- ‚úÖ API reference
- ‚úÖ Architecture overview
- ‚úÖ Provider guides

---

### Task 8.2: Create Examples

**Files in `examples/`**:
- `basic_chat.py`
- `streaming.py`
- `tool_calling.py`
- `agent_loop.py`
- `rag_pipeline.py`

**Acceptance Criteria**:
- ‚úÖ 10+ working examples
- ‚úÖ Well-commented code
- ‚úÖ README for each example

---

## Phase 9: Release

**Duration**: 1 week

### Task 9.1: Final Quality Checks

**Steps**:
1. Run full test suite
2. Check test coverage >85%
3. Run mypy strict mode
4. Run linting
5. Manual testing of all examples

**Acceptance Criteria**:
- ‚úÖ All tests passing
- ‚úÖ Coverage >85%
- ‚úÖ No type errors
- ‚úÖ No linting errors

---

### Task 9.2: Publish to PyPI

**Steps**:
1. Update version to 0.1.0
2. Build package: `poetry build`
3. Test on TestPyPI first
4. Publish to PyPI: `poetry publish`
5. Create GitHub release
6. Tag v0.1.0

**Acceptance Criteria**:
- ‚úÖ Package on PyPI
- ‚úÖ Installable via pip
- ‚úÖ GitHub release created

---

## Task Execution Guidelines

### Before Starting Any Task

1. **Read the reference docs**:
   - PYTHON_PORT_GUIDE.md for detailed implementation
   - PYTHON_PORT_QUICK_REFERENCE.md for patterns
   
2. **Check dependencies**:
   - Ensure prerequisite tasks are complete
   - Verify required modules exist

3. **Review acceptance criteria**:
   - Understand what "done" looks like
   - Plan your implementation approach

### During Implementation

1. **Follow the templates**:
   - Use code templates from PYTHON_PORT_GUIDE.md
   - Maintain consistent style

2. **Write tests first** (TDD):
   - Write failing tests
   - Implement code to pass tests
   - Refactor

3. **Type hints are mandatory**:
   - Every function must have complete type hints
   - Run mypy frequently

4. **Document as you go**:
   - Write docstrings for all public APIs
   - Include examples in docstrings

### After Completing Task

1. **Run validation commands**:
   ```bash
   poetry run pytest tests/unit/test_<module>.py -v
   poetry run mypy src/conduit/<module>
   poetry run ruff check src/conduit/<module>
   ```

2. **Check coverage**:
   ```bash
   poetry run pytest --cov=conduit/<module> --cov-report=term-missing
   ```

3. **Update progress**:
   - Mark task as complete in roadmap
   - Document any deviations or issues

---

## Validation & Testing

### Unit Tests

**Location**: `tests/unit/`

**Requirements**:
- Test each function/method independently
- Mock external dependencies
- >90% coverage for core modules
- >85% coverage overall

**Example**:
```python
@pytest.mark.asyncio
async def test_chat_success(mock_model):
    """Test successful chat."""
    response = await mock_model.chat([
        Message(role="user", content="Hi")
    ])
    assert response.content == "Mock response"
    assert mock_model.call_count == 1
```

### Integration Tests

**Location**: `tests/integration/`

**Requirements**:
- Test multiple components together
- Mock HTTP calls (use pytest-httpx)
- Test error scenarios
- Test edge cases

**Example**:
```python
@pytest.mark.asyncio
async def test_openai_chat(httpx_mock):
    """Test OpenAI chat with mocked API."""
    httpx_mock.add_response(
        url="https://api.openai.com/v1/chat/completions",
        json={"choices": [{"message": {"content": "Hello"}}]}
    )
    
    model = OpenAIModel(api_key="test")
    response = await model.chat([Message(role="user", content="Hi")])
    assert response.content == "Hello"
```

### End-to-End Tests

**Location**: `tests/e2e/`

**Requirements**:
- Test complete workflows
- Use real providers (with test keys)
- Mark as slow tests
- Run separately in CI

### Type Checking

**Run frequently**:
```bash
poetry run mypy src/conduit --strict
```

**Fix all type errors before moving on**

### Linting

**Run before committing**:
```bash
poetry run ruff check src/conduit
poetry run black src/conduit --check
```

---

## Common Issues & Solutions

### Issue: Import Errors

**Problem**: Circular imports or missing dependencies

**Solution**:
- Use `from __future__ import annotations` for forward references
- Import types only in TYPE_CHECKING block
- Restructure code to avoid circular dependencies

### Issue: Async Context Manager Not Working

**Problem**: `__aenter__` and `__aexit__` not called

**Solution**:
```python
# Correct usage
async with OpenAIModel(api_key="...") as model:
    response = await model.chat(messages)

# Wrong - missing async with
model = OpenAIModel(api_key="...")
response = await model.chat(messages)  # Client not initialized
```

### Issue: Pydantic Validation Errors

**Problem**: Data doesn't match schema

**Solution**:
- Check field types match
- Use `model_validate()` for debugging
- Print validation errors: `print(e.json())`

### Issue: Test Coverage Too Low

**Problem**: Coverage below target

**Solution**:
- Add tests for error cases
- Test edge cases
- Test validation logic
- Use `--cov-report=html` to see what's missing

### Issue: Type Errors in Tests

**Problem**: mypy complains about test code

**Solution**:
- Add type hints to test functions
- Use `# type: ignore` sparingly with comment
- Create typed fixtures

---

## Progress Tracking

### Checklist Format

Use this format to track progress:

```markdown
## Phase 1: Core Foundation

- [x] Task 1.1: Error classes (2024-01-17)
- [x] Task 1.2: Schema models (2024-01-18)
- [ ] Task 1.3: Core protocols (in progress)
- [ ] Task 1.4: Utilities

**Status**: 50% complete
**Blockers**: None
**Notes**: Schema models took longer due to validation complexity
```

### Daily Updates

At end of each day:
1. Update task checklist
2. Note any blockers
3. Document decisions made
4. List questions for review

---

## Getting Help

### Reference Documents

1. **Implementation details**: PYTHON_PORT_GUIDE.md
2. **Quick patterns**: PYTHON_PORT_QUICK_REFERENCE.md
3. **Project phases**: PYTHON_PORT_ROADMAP.md
4. **Overview**: PYTHON_PORT_SUMMARY.md

### External Resources

- [Pydantic Docs](https://docs.pydantic.dev/)
- [httpx Docs](https://www.python-httpx.org/)
- [Python asyncio](https://docs.python.org/3/library/asyncio.html)
- [OpenAI API Docs](https://platform.openai.com/docs/api-reference)
- [Anthropic API Docs](https://docs.anthropic.com/)

### Code Examples

Look at examples in PYTHON_PORT_GUIDE.md:
- Module 3: Provider implementation
- Module 4: Interceptor implementation
- Module 5: Tool implementation
- Module 6: Agent implementation
- Module 7: RAG implementation

---

## Success Metrics

### Code Quality

- ‚úÖ 100% type coverage (mypy strict)
- ‚úÖ >85% test coverage overall
- ‚úÖ >90% test coverage for core
- ‚úÖ 0 linting errors
- ‚úÖ All tests passing

### Functionality

- ‚úÖ 3+ providers working
- ‚úÖ Tool calling functional
- ‚úÖ Agent loops working
- ‚úÖ RAG pipeline complete
- ‚úÖ Interceptors operational

### Documentation

- ‚úÖ Complete API docs
- ‚úÖ 10+ examples
- ‚úÖ Getting started guide
- ‚úÖ Architecture docs
- ‚úÖ Provider guides

### Release

- ‚úÖ Published to PyPI
- ‚úÖ GitHub release created
- ‚úÖ CI/CD working
- ‚úÖ README complete

---

## Final Notes

### Critical Success Factors

1. **Type safety first** - Never skip type hints
2. **Test as you go** - Don't accumulate technical debt
3. **Follow the guide** - Templates are there for a reason
4. **Document everything** - Future you will thank you
5. **Ask for help** - Reference docs are comprehensive

### Common Pitfalls

1. ‚ùå Skipping type hints "temporarily"
2. ‚ùå Writing code before tests
3. ‚ùå Not running mypy frequently
4. ‚ùå Copying Clojure patterns literally
5. ‚ùå Forgetting to close resources

### When in Doubt

1. Check PYTHON_PORT_GUIDE.md
2. Look at code templates
3. Review similar existing code
4. Run tests frequently
5. Use mypy to catch errors early

---

**Good luck! This is a well-planned project with clear documentation. Follow the plan, write tests, and you'll succeed.**

---

**Document Version**: 1.0
**Last Updated**: 2026-01-17
**Status**: Ready for Implementation
